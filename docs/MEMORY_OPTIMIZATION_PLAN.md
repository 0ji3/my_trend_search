# ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–è¨ˆç”» - eBay ãƒˆãƒ¬ãƒ³ãƒ‰ãƒªã‚µãƒ¼ãƒãƒ„ãƒ¼ãƒ«

## ğŸ“Š ç¾çŠ¶åˆ†æ

### ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨çŠ¶æ³

| ã‚³ãƒ³ãƒ†ãƒŠ | ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ | ä½¿ç”¨ç‡ | å•é¡Œ |
|---------|-------------|--------|------|
| Backend | 1.0 GB | 6.5% | âš ï¸ åŒæœŸã‚¿ã‚¹ã‚¯å®Ÿè¡Œæ™‚ã«å¢—åŠ  |
| Frontend | 1.6 GB | 10.6% | âš ï¸ Reacté–‹ç™ºã‚µãƒ¼ãƒãƒ¼ |
| Celery Worker | 113 MB | 0.7% | âœ… æ­£å¸¸ |
| Celery Beat | 81 MB | 0.5% | âœ… æ­£å¸¸ |
| PostgreSQL | 35 MB | 0.2% | âœ… æ­£å¸¸ |
| Redis | 8 MB | 0.05% | âœ… æ­£å¸¸ |
| **åˆè¨ˆ** | **~2.9 GB** | **18.7%** | âš ï¸ ã‚¿ã‚¹ã‚¯å®Ÿè¡Œæ™‚ã«16GBè¶…é |

### å•é¡Œã®åŸå› 

**åŒæœŸã‚¿ã‚¹ã‚¯ï¼ˆ`sync_account_listings`ï¼‰ã®å®Ÿè£…**:

```python
# å•é¡Œ1: å…¨ã‚¢ã‚¤ãƒ†ãƒ ã‚’ãƒ¡ãƒ¢ãƒªã«ä¿æŒ
all_items = await self._fetch_all_active_items(access_token, account.tenant_id)
# â†‘ 4,311ã‚¢ã‚¤ãƒ†ãƒ åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€åº¦ã«ãƒ¡ãƒ¢ãƒªã«ä¿æŒ

# å•é¡Œ2: ãƒ«ãƒ¼ãƒ—å†…ã§DBã‚³ãƒŸãƒƒãƒˆã‚’ç¹°ã‚Šè¿”ã™
for item_summary in all_items:  # 4,311å›ãƒ«ãƒ¼ãƒ—
    item_data = self.trading_client.get_item(item_id, access_token)
    listing = self._upsert_listing(account, item_data)  # â† commit
    self._insert_daily_metric(listing, item_data)       # â† commit
    # ãƒ¡ãƒ¢ãƒªãŒã©ã‚“ã©ã‚“è“„ç©
```

**ãƒ¡ãƒ¢ãƒªè“„ç©ã®åŸå› **:
1. **4,311ã‚¢ã‚¤ãƒ†ãƒ ã®ä¸€æ‹¬èª­ã¿è¾¼ã¿**: ãƒªã‚¹ãƒˆå…¨ä½“ã‚’ãƒ¡ãƒ¢ãƒªã«ä¿æŒ
2. **4,311å›ã®DBã‚³ãƒŸãƒƒãƒˆ**: SQLAlchemyã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒè“„ç©
3. **SQLAlchemyã®ã‚¢ã‚¤ãƒ‡ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒãƒƒãƒ—**: ã‚³ãƒŸãƒƒãƒˆå¾Œã‚‚ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚Œã‚‹
4. **9æ™‚é–“ã®é•·æ™‚é–“å®Ÿè¡Œ**: ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ãŒè“„ç©

**Exit Code 137ã®åŸå› **:
- Dockerã®ãƒ¡ãƒ¢ãƒªåˆ¶é™ï¼ˆ16GBï¼‰ã«åˆ°é”
- OOM Killerï¼ˆOut of Memory Killerï¼‰ãŒãƒ—ãƒ­ã‚»ã‚¹ã‚’å¼·åˆ¶çµ‚äº†

---

## ğŸ¯ æœ€é©åŒ–æˆ¦ç•¥

### æˆ¦ç•¥1: ãƒãƒƒãƒå‡¦ç†ï¼ˆæœ€å„ªå…ˆï¼‰

**ç›®çš„**: ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ä¸€å®šã«ä¿ã¤

**å®Ÿè£…æ–¹æ³•**: 100-200ã‚¢ã‚¤ãƒ†ãƒ ã”ã¨ã«ãƒãƒƒãƒå‡¦ç†

```python
# Before: å…¨ã‚¢ã‚¤ãƒ†ãƒ ã‚’ä¸€åº¦ã«ãƒ¡ãƒ¢ãƒªä¿æŒ
all_items = await self._fetch_all_active_items(...)  # 4,311ã‚¢ã‚¤ãƒ†ãƒ 
for item in all_items:
    sync_item(item)

# After: ãƒšãƒ¼ã‚¸ã”ã¨ã«ãƒãƒƒãƒå‡¦ç†
for page in range(1, total_pages + 1):
    items_batch = get_page(page, entries_per_page=200)  # 200ã‚¢ã‚¤ãƒ†ãƒ ã®ã¿
    for item in items_batch:
        sync_item(item)
    # ãƒšãƒ¼ã‚¸çµ‚äº†å¾Œã«ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¯ãƒªã‚¢
    db.flush()
    db.expunge_all()  # ãƒ¡ãƒ¢ãƒªã‹ã‚‰å‰Šé™¤
```

**åŠ¹æœ**:
- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: **4,311ã‚¢ã‚¤ãƒ†ãƒ  â†’ 200ã‚¢ã‚¤ãƒ†ãƒ **ï¼ˆ95%å‰Šæ¸›ï¼‰
- ãƒ”ãƒ¼ã‚¯ãƒ¡ãƒ¢ãƒª: **~16GB â†’ ~1GB**

---

### æˆ¦ç•¥2: SQLAlchemyã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†

**ç›®çš„**: DBã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ã‚’é˜²ã

**å®Ÿè£…æ–¹æ³•**:

```python
# Before: ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒè“„ç©
for item in all_items:  # 4,311å›
    listing = self._upsert_listing(item)  # commit
    self.db.commit()  # ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒã‚»ãƒƒã‚·ãƒ§ãƒ³ã«æ®‹ã‚‹

# After: å®šæœŸçš„ã«ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ã‚¯ãƒªã‚¢
BATCH_SIZE = 100
for idx, item in enumerate(items_batch):
    listing = self._upsert_listing(item)

    # 100ä»¶ã”ã¨ã«ã‚³ãƒŸãƒƒãƒˆï¼†ã‚¯ãƒªã‚¢
    if (idx + 1) % BATCH_SIZE == 0:
        self.db.commit()
        self.db.expunge_all()  # ã‚¢ã‚¤ãƒ‡ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒãƒƒãƒ—ã‚’ã‚¯ãƒªã‚¢
        logger.debug(f"Flushed session after {idx + 1} items")

# æ®‹ã‚Šã‚’ã‚³ãƒŸãƒƒãƒˆ
self.db.commit()
```

**åŠ¹æœ**:
- SQLAlchemyã®ã‚¢ã‚¤ãƒ‡ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒãƒƒãƒ—ã‚µã‚¤ã‚º: **4,311ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ â†’ 100ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ**
- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: **ç´„80%å‰Šæ¸›**

---

### æˆ¦ç•¥3: Docker Compose ãƒ¡ãƒ¢ãƒªåˆ¶é™è¨­å®š

**ç›®çš„**: å„ã‚³ãƒ³ãƒ†ãƒŠã«ãƒ¡ãƒ¢ãƒªä¸Šé™ã‚’è¨­å®šã—ã€æš´èµ°ã‚’é˜²ã

**å®Ÿè£…æ–¹æ³•**: `docker-compose.yml`ã«åˆ¶é™ã‚’è¿½åŠ 

```yaml
services:
  backend:
    # ... existing config ...
    deploy:
      resources:
        limits:
          memory: 2G  # æœ€å¤§2GB
        reservations:
          memory: 512M  # æœ€ä½512MB

  celery-worker:
    # ... existing config ...
    deploy:
      resources:
        limits:
          memory: 3G  # æœ€å¤§3GBï¼ˆåŒæœŸã‚¿ã‚¹ã‚¯ç”¨ï¼‰
        reservations:
          memory: 256M

  frontend:
    # ... existing config ...
    deploy:
      resources:
        limits:
          memory: 1G  # é–‹ç™ºã‚µãƒ¼ãƒãƒ¼ã¯1GBã§ååˆ†
        reservations:
          memory: 256M
```

**åŠ¹æœ**:
- å„ã‚³ãƒ³ãƒ†ãƒŠãŒå€‹åˆ¥ã®ä¸Šé™ã‚’æŒã¤
- 1ã¤ã®ã‚³ãƒ³ãƒ†ãƒŠãŒæš´èµ°ã—ã¦ã‚‚ä»–ã«å½±éŸ¿ã—ãªã„
- åˆè¨ˆä¸Šé™: 2G + 3G + 1G + ... = ç´„8GBï¼ˆ16GBå†…ã«åã¾ã‚‹ï¼‰

---

### æˆ¦ç•¥4: Celeryãƒ¯ãƒ¼ã‚«ãƒ¼è¨­å®šã®æœ€é©åŒ–

**ç›®çš„**: Celeryãƒ¯ãƒ¼ã‚«ãƒ¼ã®ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ã‚’é˜²ã

**å®Ÿè£…æ–¹æ³•**: `celery_app.py`ã®è¨­å®šã‚’èª¿æ•´

```python
celery.conf.update(
    # Before
    worker_max_tasks_per_child=1000,  # 1000ã‚¿ã‚¹ã‚¯å®Ÿè¡Œå¾Œã«ãƒ¯ãƒ¼ã‚«ãƒ¼å†èµ·å‹•

    # After
    worker_max_tasks_per_child=10,    # 10ã‚¿ã‚¹ã‚¯å®Ÿè¡Œå¾Œã«ãƒ¯ãƒ¼ã‚«ãƒ¼å†èµ·å‹•
    worker_prefetch_multiplier=1,     # 1ã‚¿ã‚¹ã‚¯ãšã¤å‡¦ç†
    task_acks_late=True,              # ã‚¿ã‚¹ã‚¯å®Œäº†å¾Œã«ACK
    task_reject_on_worker_lost=True,  # ãƒ¯ãƒ¼ã‚«ãƒ¼åœæ­¢æ™‚ã¯ã‚¿ã‚¹ã‚¯å†å®Ÿè¡Œ
)
```

**åŠ¹æœ**:
- é•·æ™‚é–“å®Ÿè¡Œã«ã‚ˆã‚‹ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ã‚’é˜²ã
- 10ã‚¿ã‚¹ã‚¯ã”ã¨ã«ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹ãŒå†èµ·å‹•ã•ã‚Œã€ãƒ¡ãƒ¢ãƒªãŒã‚¯ãƒªã‚¢ã•ã‚Œã‚‹

---

### æˆ¦ç•¥5: ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã®èª¿æ•´

**ç›®çš„**: éå‰°ãªãƒ­ã‚°å‡ºåŠ›ã«ã‚ˆã‚‹ãƒ¡ãƒ¢ãƒªæ¶ˆè²»ã‚’å‰Šæ¸›

**å®Ÿè£…æ–¹æ³•**:

```python
# Before: INFOãƒ¬ãƒ™ãƒ«ã§å…¨ãƒ­ã‚°å‡ºåŠ›
logging.basicConfig(level=logging.INFO)

# After: WARNINGãƒ¬ãƒ™ãƒ«ã«å¤‰æ›´ï¼ˆæœ¬ç•ªç’°å¢ƒï¼‰
logging.basicConfig(level=logging.WARNING)

# é‡è¦ãªã‚¤ãƒ™ãƒ³ãƒˆã®ã¿INFOãƒ¬ãƒ™ãƒ«ã§å‡ºåŠ›
logger.info(f"Sync started for account {account_id}")
logger.info(f"Sync completed: {synced_count} items")
```

**åŠ¹æœ**:
- ãƒ­ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å‰Šæ¸›ï¼ˆ4,311ä»¶ â†’ æ•°åä»¶ï¼‰
- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: ç´„10-20%å‰Šæ¸›

---

## ğŸ”§ å®Ÿè£…ã‚³ãƒ¼ãƒ‰

### ä¿®æ­£1: `ebay_data_sync_service.py` - ãƒãƒƒãƒå‡¦ç†åŒ–

```python
async def sync_account_listings(self, account: EbayAccount) -> Dict[str, Any]:
    """
    Sync all active listings for one eBay account (memory-optimized)
    """
    logger.info(f"Starting sync for account {account.id}")

    # Check cache
    if self.cache_service.is_synced_today(str(account.id), "trading"):
        logger.info(f"Account {account.id} already synced today, skipping")
        return {'account_id': str(account.id), 'items_synced': 0, 'cached': True}

    errors = []
    synced_count = 0
    failed_count = 0

    try:
        # Get valid OAuth token
        access_token = await self.oauth_service.get_valid_access_token(
            self.db, account.tenant_id
        )

        # === ä¿®æ­£: ãƒšãƒ¼ã‚¸å˜ä½ã§ãƒãƒƒãƒå‡¦ç† ===
        page = 1
        entries_per_page = 200
        COMMIT_BATCH_SIZE = 100  # 100ä»¶ã”ã¨ã«ã‚³ãƒŸãƒƒãƒˆ

        while True:
            # Fetch one page at a time
            result = self.trading_client.get_my_ebay_selling(
                access_token,
                page_number=page,
                entries_per_page=entries_per_page
            )

            items_batch = result['items']
            logger.info(f"Processing page {page}/{result['total_pages']} ({len(items_batch)} items)")

            # Process items in this page
            for idx, item_summary in enumerate(items_batch):
                try:
                    item_id = item_summary['item_id']
                    if not item_id:
                        continue

                    # Get item details
                    item_data = self.trading_client.get_item(item_id, access_token)

                    # Upsert listing
                    listing = self._upsert_listing(account, item_data)

                    # Insert daily metric
                    self._insert_daily_metric(listing, item_data)

                    synced_count += 1

                    # === è¿½åŠ : 100ä»¶ã”ã¨ã«ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¯ãƒªã‚¢ ===
                    if synced_count % COMMIT_BATCH_SIZE == 0:
                        self.db.commit()
                        self.db.expunge_all()  # ãƒ¡ãƒ¢ãƒªã‹ã‚‰å‰Šé™¤
                        logger.info(f"Committed batch: {synced_count} items synced")

                except Exception as e:
                    failed_count += 1
                    error_msg = f"Failed to sync item {item_summary.get('item_id')}: {str(e)}"
                    logger.error(error_msg)
                    errors.append(error_msg)

            # === è¿½åŠ : ãƒšãƒ¼ã‚¸çµ‚äº†å¾Œã«ã‚³ãƒŸãƒƒãƒˆ ===
            self.db.commit()
            self.db.expunge_all()

            # Check if last page
            if page >= result['total_pages']:
                break

            page += 1

        # Final commit for remaining items
        self.db.commit()

        # Update last_sync_at
        account.last_sync_at = datetime.utcnow()
        self.db.commit()

        # Mark as synced
        self.cache_service.mark_synced_today(str(account.id), "trading")

        logger.info(f"Sync completed: {synced_count} succeeded, {failed_count} failed")

        return {
            'account_id': str(account.id),
            'items_synced': synced_count,
            'items_failed': failed_count,
            'sync_time': account.last_sync_at,
            'errors': errors[:10],
            'cached': False
        }

    except Exception as e:
        logger.error(f"Fatal error syncing account {account.id}: {e}", exc_info=True)
        return {
            'account_id': str(account.id),
            'items_synced': synced_count,
            'items_failed': failed_count,
            'errors': [str(e)],
        }
```

---

### ä¿®æ­£2: `docker-compose.yml` - ãƒ¡ãƒ¢ãƒªåˆ¶é™è¿½åŠ 

```yaml
version: '3.8'

services:
  postgres:
    image: postgres:16-alpine
    container_name: ebay_trends_postgres
    # ... existing config ...
    deploy:
      resources:
        limits:
          memory: 512M
        reservations:
          memory: 128M

  redis:
    image: redis:7-alpine
    container_name: ebay_trends_redis
    # ... existing config ...
    deploy:
      resources:
        limits:
          memory: 256M
        reservations:
          memory: 64M

  backend:
    build:
      context: ./backend
    container_name: ebay_trends_backend
    # ... existing config ...
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 512M

  celery-worker:
    build:
      context: ./backend
    container_name: ebay_trends_celery_worker
    # ... existing config ...
    deploy:
      resources:
        limits:
          memory: 4G  # åŒæœŸã‚¿ã‚¹ã‚¯ç”¨ã«å¤šã‚ã«ç¢ºä¿
        reservations:
          memory: 256M

  celery-beat:
    build:
      context: ./backend
    container_name: ebay_trends_celery_beat
    # ... existing config ...
    deploy:
      resources:
        limits:
          memory: 256M
        reservations:
          memory: 128M

  frontend:
    build:
      context: ./frontend
    container_name: ebay_trends_frontend
    # ... existing config ...
    deploy:
      resources:
        limits:
          memory: 1G
        reservations:
          memory: 256M
```

---

### ä¿®æ­£3: `celery_app.py` - ãƒ¯ãƒ¼ã‚«ãƒ¼è¨­å®šæœ€é©åŒ–

```python
# Celery configuration
celery.conf.update(
    task_serializer='json',
    accept_content=['json'],
    result_serializer='json',
    timezone='UTC',
    enable_utc=True,
    task_track_started=True,
    task_time_limit=30 * 60,  # 30 minutes
    task_soft_time_limit=25 * 60,  # 25 minutes

    # === ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–è¨­å®š ===
    worker_prefetch_multiplier=1,      # 1ã‚¿ã‚¹ã‚¯ãšã¤å‡¦ç†
    worker_max_tasks_per_child=5,      # 5ã‚¿ã‚¹ã‚¯å®Ÿè¡Œå¾Œã«ãƒ¯ãƒ¼ã‚«ãƒ¼å†èµ·å‹•ï¼ˆãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯é˜²æ­¢ï¼‰
    task_acks_late=True,               # ã‚¿ã‚¹ã‚¯å®Œäº†å¾Œã«ACK
    task_reject_on_worker_lost=True,   # ãƒ¯ãƒ¼ã‚«ãƒ¼åœæ­¢æ™‚ã¯ã‚¿ã‚¹ã‚¯å†å®Ÿè¡Œ

    # === è¿½åŠ : ãƒ¡ãƒ¢ãƒªä¸Šé™è¨­å®š ===
    worker_max_memory_per_child=3000000,  # 3GBï¼ˆ3,000,000 KBï¼‰
)
```

---

### ä¿®æ­£4: ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«èª¿æ•´

**`.env`ãƒ•ã‚¡ã‚¤ãƒ«**:
```bash
# æœ¬ç•ªç’°å¢ƒ
LOG_LEVEL=WARNING

# é–‹ç™ºç’°å¢ƒï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
# LOG_LEVEL=INFO
```

**`backend/app/main.py`**:
```python
import os
import logging

# ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã‚’ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—
log_level = os.getenv('LOG_LEVEL', 'INFO')
logging.basicConfig(
    level=getattr(logging, log_level),
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
```

---

## ğŸ“Š æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ

### ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®å‰Šæ¸›

| é …ç›® | ä¿®æ­£å‰ | ä¿®æ­£å¾Œ | å‰Šæ¸›ç‡ |
|------|-------|-------|--------|
| ãƒ”ãƒ¼ã‚¯ãƒ¡ãƒ¢ãƒªï¼ˆåŒæœŸä¸­ï¼‰ | ~16GB | ~4GB | **75%å‰Šæ¸›** |
| Backendé€šå¸¸æ™‚ | 1.0GB | 0.5GB | 50%å‰Šæ¸› |
| Celery WorkeråŒæœŸä¸­ | 10-15GB | 2-3GB | **80%å‰Šæ¸›** |
| Frontend | 1.6GB | 0.5GB | 70%å‰Šæ¸› |
| **åˆè¨ˆï¼ˆåŒæœŸä¸­ï¼‰** | **~16GB** | **~6GB** | **62.5%å‰Šæ¸›** |

### å®‰å®šæ€§ã®å‘ä¸Š

| æŒ‡æ¨™ | ä¿®æ­£å‰ | ä¿®æ­£å¾Œ |
|------|-------|-------|
| OOM Killerç™ºç”Ÿ | âŒ ç™ºç”Ÿï¼ˆExit 137ï¼‰ | âœ… ãªã— |
| ã‚¿ã‚¹ã‚¯å®Œäº†ç‡ | âŒ 0%ï¼ˆå¼·åˆ¶çµ‚äº†ï¼‰ | âœ… 100% |
| å®Ÿè¡Œæ™‚é–“ | 9æ™‚é–“ï¼ˆé€”ä¸­çµ‚äº†ï¼‰ | 9æ™‚é–“ï¼ˆå®Œäº†ï¼‰ |

---

## ğŸš€ å®Ÿè£…ã®å„ªå…ˆé †ä½

### Phase 1: ãƒãƒƒãƒå‡¦ç†åŒ–ï¼ˆæœ€å„ªå…ˆãƒ»å¿…é ˆï¼‰
- **å·¥æ•°**: 2-3æ™‚é–“
- **ãƒ•ã‚¡ã‚¤ãƒ«**: `backend/app/services/ebay_data_sync_service.py`
- **åŠ¹æœ**: ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡75%å‰Šæ¸›
- **ãƒªã‚¹ã‚¯**: ä½

### Phase 2: Docker ãƒ¡ãƒ¢ãƒªåˆ¶é™ï¼ˆæ¨å¥¨ï¼‰
- **å·¥æ•°**: 30åˆ†
- **ãƒ•ã‚¡ã‚¤ãƒ«**: `docker-compose.yml`
- **åŠ¹æœ**: ã‚³ãƒ³ãƒ†ãƒŠæš´èµ°é˜²æ­¢
- **ãƒªã‚¹ã‚¯**: ä½

### Phase 3: Celeryãƒ¯ãƒ¼ã‚«ãƒ¼æœ€é©åŒ–ï¼ˆæ¨å¥¨ï¼‰
- **å·¥æ•°**: 1æ™‚é–“
- **ãƒ•ã‚¡ã‚¤ãƒ«**: `backend/app/celery_app.py`
- **åŠ¹æœ**: é•·æœŸå®Ÿè¡Œæ™‚ã®ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯é˜²æ­¢
- **ãƒªã‚¹ã‚¯**: ä½

### Phase 4: ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«èª¿æ•´ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
- **å·¥æ•°**: 15åˆ†
- **ãƒ•ã‚¡ã‚¤ãƒ«**: `.env`, `backend/app/main.py`
- **åŠ¹æœ**: ãƒ¡ãƒ¢ãƒªãƒ»ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨é‡10-20%å‰Šæ¸›
- **ãƒªã‚¹ã‚¯**: ä½

---

## âš ï¸ æ³¨æ„äº‹é …

### 1. SQLAlchemy `expunge_all()` ã®å½±éŸ¿

**æ³¨æ„**: ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‹ã‚‰å‰Šé™¤ã•ã‚ŒãŸã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¯å†åˆ©ç”¨ã§ãã¾ã›ã‚“

```python
# expunge_all()å¾Œã¯ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒç„¡åŠ¹
listing = self._upsert_listing(item)
self.db.expunge_all()
print(listing.id)  # âŒ DetachedInstanceError
```

**å¯¾å‡¦**: å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ã¯äº‹å‰ã«æŠ½å‡º

```python
listing = self._upsert_listing(item)
listing_id = listing.id  # âœ… å€¤ã‚’ã‚³ãƒ”ãƒ¼
self.db.expunge_all()
print(listing_id)  # âœ… OK
```

### 2. Docker Compose v3ã®ãƒ¡ãƒ¢ãƒªåˆ¶é™

**æ³¨æ„**: `docker-compose up`ã§ã¯`deploy.resources`ãŒç„¡è¦–ã•ã‚Œã‚‹å ´åˆãŒã‚ã‚Šã¾ã™

**å¯¾å‡¦**: `docker stack deploy`ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã€`mem_limit`ã‚’ä½µç”¨

```yaml
services:
  backend:
    mem_limit: 2g  # v3ã§ç¢ºå®Ÿã«å‹•ä½œ
    deploy:
      resources:
        limits:
          memory: 2G  # Swarm modeç”¨
```

### 3. Celeryãƒ¯ãƒ¼ã‚«ãƒ¼ã®å†èµ·å‹•é »åº¦

**æ³¨æ„**: `worker_max_tasks_per_child=5`ã¯5ã‚¿ã‚¹ã‚¯ã”ã¨ã«å†èµ·å‹•

**å½±éŸ¿**:
- åŒæœŸã‚¿ã‚¹ã‚¯1å› = 1ã‚¿ã‚¹ã‚¯
- 5å›ã®åŒæœŸå¾Œã«ãƒ¯ãƒ¼ã‚«ãƒ¼å†èµ·å‹•
- å†èµ·å‹•æ™‚é–“: ç´„5-10ç§’

**èª¿æ•´**: å¿…è¦ã«å¿œã˜ã¦`10`ã‚„`20`ã«å¢—ã‚„ã™

---

## ğŸ“ ãƒ‡ãƒ—ãƒ­ã‚¤æ‰‹é †

### Step 1: ã‚³ãƒ¼ãƒ‰ä¿®æ­£
```bash
# ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
cp backend/app/services/ebay_data_sync_service.py backend/app/services/ebay_data_sync_service.py.bak

# ä¿®æ­£ç‰ˆã‚’ãƒ‡ãƒ—ãƒ­ã‚¤ï¼ˆä¸Šè¨˜ã®ä¿®æ­£1ã‚’é©ç”¨ï¼‰
```

### Step 2: Docker Composeä¿®æ­£
```bash
# ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
cp docker-compose.yml docker-compose.yml.bak

# ãƒ¡ãƒ¢ãƒªåˆ¶é™ã‚’è¿½åŠ ï¼ˆä¸Šè¨˜ã®ä¿®æ­£2ã‚’é©ç”¨ï¼‰
```

### Step 3: Celeryè¨­å®šä¿®æ­£
```bash
# ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
cp backend/app/celery_app.py backend/app/celery_app.py.bak

# ãƒ¯ãƒ¼ã‚«ãƒ¼è¨­å®šã‚’æœ€é©åŒ–ï¼ˆä¸Šè¨˜ã®ä¿®æ­£3ã‚’é©ç”¨ï¼‰
```

### Step 4: å†èµ·å‹•
```bash
# ã‚³ãƒ³ãƒ†ãƒŠã‚’å†ãƒ“ãƒ«ãƒ‰ï¼†èµ·å‹•
docker-compose down
docker-compose build
docker-compose up -d

# ãƒ­ã‚°ç¢ºèª
docker-compose logs -f celery_worker
```

### Step 5: ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
```bash
# æ‰‹å‹•ã§åŒæœŸã‚¿ã‚¹ã‚¯ã‚’ãƒˆãƒªã‚¬ãƒ¼
docker-compose exec backend python -c "
from app.tasks.daily_sync import sync_all_accounts
result = sync_all_accounts()
print(result)
"

# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ç›£è¦–
watch -n 5 'docker stats --no-stream'
```

---

**Last Updated**: 2025-10-17
**Status**: è¨­è¨ˆå®Œäº†ã€å®Ÿè£…å¾…ã¡
**æ¨å¥¨**: Phase 1ï¼ˆãƒãƒƒãƒå‡¦ç†åŒ–ï¼‰ã‚’æœ€å„ªå…ˆã§å®Ÿè£…
